{evaluate_tf_completionevaluate_tf_completion
 "cells": [
  {evaluate_tf_completion
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "nameevaluate_tf_completion
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import random as r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "import oai_utils as ou\n",
    "import evaluation as ev\n",
    "import dataset_gen as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate = False\n",
    "if regenerate:\n",
    "    def generate_eqn_true(seed: int):\n",
    "        r.seed(seed)\n",
    "        first_int = r.randint(1, 100)\n",
    "        second_int = r.randint(1, 100)\n",
    "        sum = first_int + second_int\n",
    "        return f\"{first_int} + {second_int} = {sum}\"\n",
    "\n",
    "    def generate_eqn_false(seed: int):\n",
    "        r.seed(seed)\n",
    "        first_int = r.randint(1, 100)\n",
    "        second_int = r.randint(1, 100)\n",
    "        random_sign = r.choice([+1, -1])\n",
    "        sum = first_int + second_int + random_sign * r.randint(1, 5)\n",
    "        return f\"{first_int} + {second_int} = {sum}\"\n",
    "\n",
    "    N_total_each = 200\n",
    "    seeds = range(N_total_each)\n",
    "    eqns_true = [generate_eqn_true(seed) for seed in seeds]\n",
    "    eqns_false = [generate_eqn_false(seed) for seed in seeds]\n",
    "    dataset_sums = ds.Dataset(\n",
    "        desc='sums0',\n",
    "        pos_examples=eqns_true,\n",
    "        neg_examples=eqns_false,\n",
    "    )\n",
    "    dataset_sums.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sums = ds.Dataset.load(\"sums0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test evaluation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before setting temperature to zero had:\n",
    "```\n",
    "label\n",
    "False    0.6\n",
    "True     0.9\n",
    "Name: correct, dtype: float64\n",
    "```\n",
    "\n",
    "Afterwards had lower accuracy:\n",
    "```\n",
    "label\n",
    "False    0.6\n",
    "True     0.9\n",
    "Name: correct, dtype: float64\n",
    "```\n",
    "\n",
    "But this seems like 'the right' thing to do, so will stick with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deprecated this function\n",
    "# \n",
    "# outs = ev.evaluate_api(\n",
    "#     dataset=ds_sums,\n",
    "#     n_egs=5,\n",
    "#     seeds=range(10),\n",
    "#     max_tokens=10,\n",
    "#     json_format=True\n",
    "# )\n",
    "# perfs = pd.DataFrame(outs)\n",
    "# display(perfs.head())\n",
    "# perfs['correct'] = (perfs['label'] == perfs['response'])\n",
    "# sum_stats = perfs.groupby('label')['correct'].mean()\n",
    "# # # did a quick check of reproducibility with this version\n",
    "# # # defined analogous outs2 then\n",
    "# # # # outs == outs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mini, performance could be improved\n",
    "```\n",
    "label  n_egs\n",
    "False  5        0.35\n",
    "       10       0.45\n",
    "       20       0.75\n",
    "True   5        0.80\n",
    "       10       0.95\n",
    "       20       0.65\n",
    "Name: correct, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = ev.evaluate_api(\n",
    "    dataset_desc='sums0',\n",
    "    n_eg_options=[10, 20, 40],\n",
    "    num_seeds=40,\n",
    "    max_tokens=10,\n",
    "    json_format=True,\n",
    "    model='gpt-4o',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>n_egs</th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  n_egs  label  response\n",
       "0     0     10   True      True\n",
       "1     0     10  False     False\n",
       "2     1     10   True      True\n",
       "3     1     10  False     False\n",
       "4     2     10   True      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perfs = pd.DataFrame(outs)\n",
    "display(perfs.head())\n",
    "perfs['correct'] = (perfs['label'] == perfs['response'])\n",
    "sum_stats = perfs.groupby(['label', 'n_egs'])['correct'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  n_egs\n",
       "False  10       0.750\n",
       "       20       0.750\n",
       "       40       0.675\n",
       "True   10       0.875\n",
       "       20       0.875\n",
       "       40       0.800\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sum_stats.xs(True, level='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making problem easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_gen as dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_each=200\n",
    "max_int=10\n",
    "max_diff=2\n",
    "\n",
    "dg.gen_sum_ds(N_total_each, max_int, max_diff)\n",
    "sum_ds = dg.load_sum_ds(N_total_each, max_int, max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sums_maxint_10_maxdiff_2_N_200'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ds.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = ev.evaluate_api(\n",
    "    dataset_desc=sum_ds.desc,\n",
    "    n_eg_options=[10, 20, 40],\n",
    "    num_seeds=40,\n",
    "    max_tokens=10,\n",
    "    json_format=True,\n",
    "    model='gpt-4o',\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>n_egs</th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  n_egs  label  response\n",
       "0     0     10   True      True\n",
       "1     0     10  False     False\n",
       "2     1     10   True      True\n",
       "3     1     10  False     False\n",
       "4     2     10   True     False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perfs = pd.DataFrame(outs)\n",
    "display(perfs.head())\n",
    "perfs['correct'] = (perfs['label'] == perfs['response'])\n",
    "sum_stats = perfs.groupby(['label', 'n_egs'])['correct'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label  n_egs\n",
       "False  10       0.900\n",
       "       20       0.950\n",
       "       40       0.900\n",
       "True   10       0.950\n",
       "       20       1.000\n",
       "       40       0.975\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that there does seem to be a sweetspot of 20 examples.  \n",
    "Also a predispension towards saying True more than False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing types of numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return random integer in range [a, b], including both end points.\n",
      "        \n",
      "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.8/random.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "r.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "primes = dg.calculate_first_200_primes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(primes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcum_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Return a k sized list of population elements chosen with replacement.\n",
      "\n",
      "If the relative weights or cumulative weights are not specified,\n",
      "the selections are made with equal probability.\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/lib/python3.8/random.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?r.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_each=200\n",
    "seed=0\n",
    "max_int=50\n",
    "rule='even'\n",
    "\n",
    "\n",
    "ds = dg.get_number_rule_dataset(N_total_each, seed, max_int, rule)\n",
    "print(ds.desc)\n",
    "outs = ev.evaluate_api(\n",
    "    dataset_desc=ds.desc,\n",
    "    n_eg_options=[5, 10, 20, 40],\n",
    "    num_seeds=40,\n",
    "    max_tokens=10,\n",
    "    json_format=True,\n",
    "    model='gpt-4o-mini',\n",
    ") \n",
    "perfs = pd.DataFrame(outs)\n",
    "display(perfs.head())\n",
    "perfs['correct'] = (perfs['label'] == perfs['response'])\n",
    "sum_stats = perfs.groupby(['label', 'n_egs'])['correct'].mean()\n",
    "\n",
    "sum_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
